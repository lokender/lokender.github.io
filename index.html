<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Dr. Lokender  Tiwari, Ph.D.</title>
    <meta name="author" content="Dr. Lokender  Tiwari, Ph.D.">
    <meta name="description" content="Dr. Lokender Tiwari, PhD
">
    <meta name="keywords" content="Lokender Tiwari, Dr. Lokender Tiwari, Computer Vision, Machine Learning, Artificial Intelligence, Deep Learning, Computer Graphics, Pseudo RGBD, NEC , IIIT Delhi, IIIT, TCS, TCS Research, Research, Cloth Animation, Metaverse, DGSAC, DeepDraper, GarSim, Lokendra, 3D Vision, 3D Computer Vision">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!-- Social Icons -->
          <div class="navbar-brand social">
            <a href="https://scholar.google.com/citations?user=uHfnun4AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/lokender" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/lokender" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/lokendertiwari" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            

          </div>
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">Home<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Selected Projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/photos/">Photos</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Dr. Lokender</span>  Tiwari, Ph.D.
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/p1-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/p1-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/p1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/p1.png" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="p1.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I am a Research Scientist at Visual Computing Group at <a href="https://www.tcs.com/tcs-research" rel="external nofollow noopener" target="_blank">TCS Research</a>. Prior to joining TCS Research, I worked at <a href="https://www.nec-labs.com/research-departments/media-analytics/media-analytics-people" rel="external nofollow noopener" target="_blank">NEC Laboratories America</a>, with <a href="https://www.nec-labs.com/manmohan-chandraker" rel="external nofollow noopener" target="_blank">Prof. Manmohan Chandraker</a>, <a href="https://cs.adelaide.edu.au/~huy/home.php?id=start" rel="external nofollow noopener" target="_blank">Dr. Quoc-Huy Tran</a>, and <a href="https://sites.google.com/view/panji530" rel="external nofollow noopener" target="_blank">Dr. Pan Ji</a> on <a href="https://lokender.github.io/self-improving-SLAM.html" rel="external nofollow noopener" target="_blank">self-improving framework for SLAM</a>. I received my PhD from <a href="https://www.iiitd.edu.in/" rel="external nofollow noopener" target="_blank">IIIT-Delhi</a>, with the Doctoral Dissertation Award. I was advised by <a href="https://faculty.iiitd.ac.in/~anands/index.html/" rel="external nofollow noopener" target="_blank">Dr. Saket Anand</a> and funded by prestegious <a href="https://phd.dic.gov.in/" rel="external nofollow noopener" target="_blank">Visvesvarya PhD fellowship</a>. I am the recipient of <a href="https://lokender.github.io/gold.html" target="_blank" rel="external nofollow noopener">Chancellor’s Gold Medal</a> for academic excellence in B.Tech (Computer Science and Engineering) 2008-2012.</p>

<ul> <b>email</b> : lokender [dot] tiwari [@] tcs [dot] com , tiwarilokender [@] gmail [dot] com </ul>
<ul>  <b>Research Interests</b>: I actively research in the following areas 
  <ul> <li> Geometric 3D Perception and Modelling</li>
   <li> Neural Physics Based Simulation and Synthesis </li>
   <li> Differentiable Graphics and Inverse Rendering </li>
   <li> Generative 3D Modeling and Synthesis </li>
   <li> Physics-Aware AR/VR/XR </li>
   <li> Robust and Reliable Deep Learning </li> </ul>
   </ul>
<p><a href="/#news">news</a>   |   <a href="/#talks">talks</a>   |   <a href="/#awards">awards</a>   |   <a href="/#publications">publications</a>   |   <a href="/#patents">patents</a>   |  <a href="/#mentoring">mentoring</a></p>

<hr>

<h2 id="news">News</h2>
<ul> 
<li>[2022] One US Patent #11,468,585 granted. </li>
  <li>[2022] One paper accepted to <a href="https://wacv2023.thecvf.com/home" rel="external nofollow noopener" target="_blank"> IEEE WACV, 2023</a> </li>
  <li>[2022] Serving Programm Committee Member for the <a href="https://aaai.org/Conferences/AAAI-23/" rel="external nofollow noopener" target="_blank"> AAAI, 2023</a> </li>
  <li>[2022] Serving Technical Programm Committee Member for the <a href="https://www.comsnets.org/cvad_workshop.html" rel="external nofollow noopener" target="_blank"> Connected Vehicles and Autonomous Driving </a> at <a href="https://www.comsnets.org/" rel="external nofollow noopener" target="_blank"> COMSNETS, 2023</a> </li>
   <details>
  <summary>Show more...</summary>
  <li>[2022] One paper accepted to <a href="https://wacv2022.thecvf.com/home" rel="external nofollow noopener" target="_blank"> IEEE WACV, 2022</a> </li>
  <li>[2021] 3D Virtual Garment Try-on (DeepDraper) accepted in the <a href="http://iccv2021.thecvf.com/" rel="external nofollow noopener" target="_blank"> ICCV, 2021 </a> <a href="http://montrealrobotics.ca/diff3d/" rel="external nofollow noopener" target="_blank"> Workshop</a> </li>
  <li>[2021] Keynote talk on <i>Robust Scene Understanding</i> at the <a href="https://www.bitdurg.ac.in/bitcon2021/keynotes.html" rel="external nofollow noopener" target="_blank">BITCON, 2021</a>
</li>
  <li>[2020] Self-improving SLAM and Depth Predictions accepted in <a href="https://eccv2020.eu/" target="_blank" rel="external nofollow noopener">ECCV 2020</a> </li>
  <li>[2020] <i>REGroup</i>. A test time replacement of Softmax for robust predictions, <a href="https://lokender.github.io/REGroup.html" target="_blank" rel="external nofollow noopener">Project Page</a>
</li> 
</details>
</ul>

<hr>

<h2 id="talks">Invited Talks/Keynotes</h2>
<ul> <li> [<b>Jan, 2023</b>] Invited talk on <i>Robust Scene Understanding</i>  at  <a href="https://lu.ma/kdmdelhincr43" rel="external nofollow noopener" target="_blank"> 43rd Kaggle Days Meetup Delhi-NCR</a> </li>
<li> [<b>Nov, 2022</b>] Invited talk on <i>Simultaneous Localization and Mapping (Classical to Leaning Based)</i>  at  <a href="https://www.cse.iitd.ac.in" rel="external nofollow noopener" target="_blank"> IIT-Delhi</a> hosted by <a href="https://www.cse.iitd.ac.in/~chetan/" rel="external nofollow noopener" target="_blank"> </a> Prof. Chetan Arora </li>
<li> [<b>Oct, 2022</b>] Invited talk on <i>Metaverse: Current and The Future, a Visual Computing Perspective</i>  at  <a href="https://sushantuniversity.edu.in/school-of-engineering-and-technology" rel="external nofollow noopener" target="_blank"> Sushant University</a>
</li>
<li> [<b>Oct, 2021</b>] Keynote talk on <i>Scene Understanding</i>  at  <a href="https://www.bitdurg.ac.in/bitcon/bitcon2021/keynotes.html" rel="external nofollow noopener" target="_blank"> BITCON 2021, BITS Durg</a>
</li>
</ul>

<hr>

<h2 id="awards">Awards</h2>
<ul> <li> Received <a href="https://www.flipbookpdf.net/web/site/593a0d5023e74e15c0b902e4cd702ae406d83c0aFBP23162779.pdf.html#page/33" rel="external nofollow noopener" target="_blank"> Ph.D. Dissertation Award, 2021</a>
</li>
<li>
<a href="http://wacv18.uccs.us/" target="_blank" rel="external nofollow noopener">IEEE WACV 2018</a> Ph.D. Forum Award. <b>Mentor:</b> Prof. <a href="https://vast.uccs.edu/~tboult/" target="_blank" rel="external nofollow noopener">Terrance Boult</a>
</li>
<li>Best Doctoral Symposium Award (3rd), <a href="https://www.iitg.ac.in/icvgip2016/BestPaperAwards.php" target="_blank" rel="external nofollow noopener">ICVGIP'2016</a>
</li>
<li>2nd Prize, Poster Session, Research Showcase 2017 @ <a href="https://www.iiitd.ac.in/" target="_blank" rel="external nofollow noopener">IIIT-Delhi</a>
</li>
<li>IEEE Signal Processing Society, Travel Award to attend <a href="http://www.2016.ieeeicip.org/ICIP%202016/www2.securecms.com/ICIP2016/default.html" target="_blank" rel="external nofollow noopener">IEEE ICIP 2016</a> </li>
<li>
<a href="https://phd.dic.gov.in/" target="_blank" rel="external nofollow noopener">Visvesvarya Ph.D. Fellowship</a> award, Jul 2014- Dec 2019</li>
<li>MHRD Graduate Fellowship, GATE (Top 4% in India), Jul 2012- Jun 2014</li>
<li>
<a href="https://lokender.github.io/gold.html" target="_blank" rel="external nofollow noopener">Chancellor's Gold Medal</a> for academic excellence in undergraduate B.Tech CSE., 2008-2012</li> </ul>

<hr>

<h3 id="publications">Publications</h3>
<div class="publications" id="publications">
  <h2 class="year">2023</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/garsim_wacv_main.gif"></div>

        <!-- Entry bib key -->
        <div id="tiwari2023garsim" class="col-sm-8">
        <!-- Title -->
        <div class="title">GarSim: Particle Based Neural Garment Simulator</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, and Brojeshwar Bhowmick</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE/CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Tiwari_GarSim_Particle_Based_Neural_Garment_Simulator_WACV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a particle-based neural garment simulator (dubbed as GarSim) that can simulate template garments on the target arbitrary body poses. Existing learning-based methods majorly work for specific garment type (e.g. t-shirt, skirt, etc) or garment topology, and needs retraining for a new type of garment. Similarly, some methods focus on a particular fabric, body shape, and pose. To circumvent these limitations, our method fundamentally learns the physical dynamics of the garment vertices conditioned on underlying body shape, motion, and fabric properties to generalize across garment types, topology, and fabric along with different body shape and pose. In particular, we represent the garment as a graph, where the nodes represent the physical state of the garment vertices, and the edges represent the relation between the two nodes. The nodes and edges of the garment graph encode various properties of garments and the human body to compute the dynamics of the vertices through a learned message-passing. Learning of such dynamics of the garment vertices conditioned on underlying body motion and fabric properties enables our method to be trained simultaneously for multiple types of garments (e.g., tops, skirts, etc) with arbitrary mesh resolutions, varying topologies, and fabric properties. Our experimental results show that GarSim with less amount of training data not only outperforms the SOTA methods on challenging CLOTH3D dataset both qualitatively and quantitatively, but also works reliably well on the unseen poses obtained from YouTube videos, and give satisfactory results on unseen cloth types which were not present during the training.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tiwari2023garsim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GarSim: Particle Based Neural Garment Simulator}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tiwari, Lokender and Bhowmick, Brojeshwar}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Winter Conference on Applications of Computer Vision (&lt;b&gt;WACV&lt;/b&gt;)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4472--4481}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/slam_graph.png"></div>

        <!-- Entry bib key -->
        <div id="gadipudi2022review" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Review on Monocular Tracking and Mapping: From Model-Based to Data-Driven Methods</div>
        <!-- Author -->
        <div class="author">
        

        Nivesh Gadipudi, Irraivan Elamvazuthi, Lila Iznita Izhar, <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Ramya Hebbalaguppe, Cheng-Kai Lu, Arockia Selvakumar Arockia Doss' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '6');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>The Visual Computer, International Journal of Computer Graphics</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://link.springer.com/article/10.1007/s00371-022-02702-z" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Visual odometry and visual simultaneous localization and mapping aid in tracking the position of a camera and mapping the surroundings using images. It is an important part of robotic perception. Tracking and mapping using a monocular camera is cost-effective, requires less calibration effort, and is easy to deploy across a wide range of applications. This paper provides an extensive review of the developments for the first two decades of the twenty-first century. Astounding results from early methods based on filtering have intrigued the community to extend these algorithms using other forms of techniques like bundle adjustment and deep learning. This article starts by introducing the basic sensor systems and analyzing the evolution of monocular tracking and mapping algorithms through bibliometric data. Then, it covers the overview of filtering and bundle adjustment methods, followed by recent advancements in methods using deep learning with the mathematical constraints applied on the networks. Finally, the popular benchmarks available for developing and evaluating these algorithms are presented along with a comparative study on a different class of algorithms. It is anticipated that this article will serve as the latest introductory tool and further ignite the interest of the community to solve current and future impediments. </p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">gadipudi2022review</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Review on Monocular Tracking and Mapping: From Model-Based to Data-Driven Methods}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gadipudi, Nivesh and Elamvazuthi, Irraivan and Izhar, Lila Iznita and Tiwari, Lokender and Hebbalaguppe, Ramya and Lu, Cheng-Kai and Doss, Arockia Selvakumar Arockia}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Visual Computer, International Journal of Computer Graphics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--28}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/REGroup.png"></div>

        <!-- Entry bib key -->
        <div id="tiwari2022regroup" class="col-sm-8">
        <!-- Title -->
        <div class="title">REGroup: Rank-aggregating Ensemble of Generative Classifiers for Robust Predictions</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, Anish Madan, Saket Anand, and Subhashis Banerjee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE/CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>)</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Tiwari_REGroup_Rank-Aggregating_Ensemble_of_Generative_Classifiers_for_Robust_Predictions_WACV_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://openaccess.thecvf.com/content/WACV2022/supplemental/Tiwari_REGroup_Rank-Aggregating_Ensemble_WACV_2022_supplemental.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a>
            <a href="https://github.com/lokender/REGroup" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://lokender.github.io/projects/REGroup/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Deep Neural Networks (DNNs) are often criticized for being susceptible to adversarial attacks. Most successful defense strategies adopt adversarial training or random input transformations that typically require retraining or fine-tuning the model to achieve reasonable performance. In this work, our investigations of intermediate representations of a pre-trained DNN lead to an interesting discovery pointing to intrinsic robustness to adversarial attacks. We find that we can learn a generative classifier by statistically characterizing the neural response of an intermediate layer to clean training samples. The predictions of multiple such intermediate-layer based classifiers, when aggregated, show unexpected robustness to adversarial attacks. Specifically, we devise an ensemble of these generative classifiers that rank-aggregates their predictions via a Borda count-based consensus. Our proposed approach uses a subset of the clean training data and a pre-trained model, and yet is agnostic to network architectures or the adversarial attack generation method. We show extensive experiments to establish that our defense strategy achieves state-of-the-art performance on the ImageNet validation set.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tiwari2022regroup</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{REGroup: Rank-aggregating Ensemble of Generative Classifiers for Robust Predictions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tiwari, Lokender and Madan, Anish and Anand, Saket and Banerjee, Subhashis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{WACV, 2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Winter Conference on Applications of Computer Vision (&lt;b&gt;WACV&lt;/b&gt;)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2595--2604}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/deepdraper.gif"></div>

        <!-- Entry bib key -->
        <div id="tiwari2021deepdraper" class="col-sm-8">
        <!-- Title -->
        <div class="title">DeepDraper: Fast and Accurate 3D Garment Draping over a 3D Human Body</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, and Brojeshwar Bhowmick</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), Differentiable 3D Vision and Graphics Workshop </em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021W/Diff3D/papers/Tiwari_DeepDraper_Fast_and_Accurate_3D_Garment_Draping_Over_a_3D_ICCVW_2021_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021W/Diff3D/supplemental/Tiwari_DeepDraper_Fast_and_ICCVW_2021_supplemental.zip" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Draping a 3D human mesh has garnered broad interest due to its wide applicability in virtual try-on, animations, etc. The 3D garment deformations produced by the existing methods are often inconsistent with the body shape, pose, and measurements. This paper proposes a single unified learning-based framework (DeepDraper) to predict garment deformation as a function of body shape, pose, measurements, and garment styles. We train the DeepDraper with coupled geometric and multi-view perceptual losses. Unlike existing methods, we additionally model garment deformations as a function of standard body measurements, which generally a buyer or a designer uses to buy or design perfect fit clothes. As a result, DeepDraper significantly outperforms the state-of-the-art deep network-based approaches in terms of fitness and realism and generalizes well to the unseen style of the garments. In addition to that, DeepDraper is 10 times smaller in size and 23 times faster than the closest state-of-the-art method (TailorNet), which favors its use in real-time applications with less computational power. Despite being trained on the static poses of the TailorNet dataset, DeepDraper generalizes well to unseen body shapes, poses, and garment styles and produces temporally coherent garment deformations on the pose sequences even from the unseen AMASS dataset.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tiwari2021deepdraper</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DeepDraper: Fast and Accurate 3D Garment Draping over a 3D Human Body}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tiwari, Lokender and Bhowmick, Brojeshwar}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF International Conference on Computer Vision (&lt;b&gt;ICCV&lt;/b&gt;), Differentiable 3D Vision and Graphics Workshop }</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1416--1426}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/prgbd_teaser.gif"></div>

        <!-- Entry bib key -->
        <div id="tiwari2020pseudo" class="col-sm-8">
        <!-- Title -->
        <div class="title">Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, Pan Ji, Quoc-Huy Tran, Bingbing Zhuang, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Saket Anand, Manmohan Chandraker' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '6');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In European conference on computer vision (<b>ECCV</b>)</em>, 2020
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2004.10681" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560426.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://lokender.github.io/projects/pseudo-rgbd-slam/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Classical monocular Simultaneous Localization And Mapping (SLAM) and the recently emerging convolutional neural networks (CNNs) for monocular depth prediction represent two largely disjoint approaches towards building a 3D map of the surrounding environment. In this paper, we demonstrate that the coupling of these two by leveraging the strengths of each mitigates the other’s shortcomings. Specifically, we propose a joint narrow and wide baseline based self-improving framework, where on the one hand the CNN-predicted depth is leveraged to perform pseudo RGB-D feature-based SLAM, leading to better accuracy and robustness than the monocular RGB SLAM baseline. On the other hand, the bundle-adjusted 3D scene structures and camera poses from the more principled geometric SLAM are injected back into the depth network through novel wide baseline losses proposed for improving the depth prediction network, which then continues to contribute towards better pose and 3D structure estimation in the next iteration. We emphasize that our framework only requires unlabeled monocular videos in both training and inference stages, and yet is able to outperform state-of-the-art self-supervised monocular and stereo depth prediction networks (e.g., Monodepth2) and feature-based monocular SLAM system (i.e., ORB-SLAM). Extensive experiments on KITTI and TUM RGB-D datasets verify the superiority of our self-improving geometry-CNN framework.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tiwari2020pseudo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tiwari, Lokender and Ji, Pan and Tran, Quoc-Huy and Zhuang, Bingbing and Anand, Saket and Chandraker, Manmohan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European conference on computer vision (&lt;b&gt;ECCV&lt;/b&gt;)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{437--455}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/dgsac_wacv.jpg"></div>

        <!-- Entry bib key -->
        <div id="tiwari2018dgsac" class="col-sm-8">
        <!-- Title -->
        <div class="title">DGSAC: Density Guided Sampling and Consensus</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, and Saket Anand</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE Winter Conference on Applications of Computer Vision ((<b>WACV</b>)</em>, 2018
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/TiwariAnand_WACV2018_DGSAC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/lokender/dgsac" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this paper, we present an automatic multi-model fitting pipeline that can robustly fit multiple geometric models present in the corrupted and noisy data. Our approach can handle large data corruption and requires no user input, unlike most state-of-the-art approaches. The pipeline can be used as an independent block in many geometric vision applications like 3D reconstruction, motion and planar segmentation. We use residual density as the primary tool to guide hypothesis generation, estimate the fraction of inliers, and perform model selection. We show results for a diverse set of geometric models like planar homographies, fundamental matrices and vanishing points, which often arise in various computer vision applications. Despite being fully automatic, our approach achieves competitive performance compared to state-of-the-art approaches in terms of accuracy and computational time.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tiwari2018dgsac</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DGSAC: Density Guided Sampling and Consensus}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tiwari, Lokender and Anand, Saket}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE Winter Conference on Applications of Computer Vision ((&lt;b&gt;WACV&lt;/b&gt;)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{974--982}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/dpa_accv.png"></div>

        <!-- Entry bib key -->
        <div id="tiwari2017robust" class="col-sm-8">
        <!-- Title -->
        <div class="title">Robust Multi-Model Fitting using Density and Preference Analysis</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, Saket Anand, and Sushil Mittal</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Asian Conference on Computer Vision (<b>ACCV</b>)</em>, 2017
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/dpa_accv2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/lokender/DPA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Robust multi-model fitting problems are often solved using consensus based or preference based methods, each of which captures largely independent information from the data. However, most existing techniques still adhere to either of these approaches. In this paper, we bring these two paradigms together and present a novel robust method for discovering multiple structures from noisy, outlier corrupted data. Our method adopts a random sampling based hypothesis generation and works on the premise that inliers are densely packed around the structure, while the outliers are sparsely spread out. We leverage consensus maximization by defining the residual density, which is a simple and efficient measure of density in the 1-D residual space. We locate the inlier-outlier boundary by using preference based point correlations together with the disparity in residual density of inliers and outliers. Finally, we employ a simple strategy that uses preference based hypothesis correlation and residual density to identify one hypothesis representing each structure and their corresponding inliers. The strength of the proposed approach is evaluated empirically by comparing with state-of-the-art techniques over synthetic data and the AdelaideRMF dataset.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tiwari2017robust</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust Multi-Model Fitting using Density and Preference Analysis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tiwari, Lokender and Anand, Saket and Mittal, Sushil}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Asian Conference on Computer Vision (&lt;b&gt;ACCV&lt;/b&gt;)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{308--323}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/fhf_icip.jpg"></div>

        <!-- Entry bib key -->
        <div id="tiwari2016fast" class="col-sm-8">
        <!-- Title -->
        <div class="title">Fast Hypothesis Filtering for Multi-Structure Geometric Model Fitting</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, and Saket Anand</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE International Conference on Image Processing (<b>ICIP</b>)</em>, 2016
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/fhf_icip2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We propose a fast and efficient two-stage hypothesis filtering technique that can improve performance of clustering based robust multi-model fitting algorithms. Sampling based hypothesis generation is nondeterministic and permits little control over generating poor model hypotheses, often leading to a significant proportion of bad hypotheses. Our novel filtering approach leverages the asymmetry in the distributions of points around the inlier/outlier boundary via the sample skewness computed in the residual space. The output is a set of promising hypotheses which aid multi-model fitting algorithms in improving accuracy as well as running time. We validate our approach on the AdelaideRMF dataset and show favorable results along with comparisons to state-of-the-art.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tiwari2016fast</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fast Hypothesis Filtering for Multi-Structure Geometric Model Fitting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tiwari, Lokender and Anand, Saket}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Image Processing (&lt;b&gt;ICIP&lt;/b&gt;)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3728--3732}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

</div>

<hr>

<h3 id="patents">Patents</h3>
<p>Go to : <a href="/#publications">Publications</a></p>
<div class="publications" id="patents">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/deepdraper_patent.png"></div>

        <!-- Entry bib key -->
        <div id="tiwari2022method" class="col-sm-8">
        <!-- Title -->
        <div class="title">Method and System for Draping a 3D Garment on a 3D Human Body</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, and Brojeshwar Bhowmick</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          Nov 2022
        </div>
        <div class="periodical">
          US Patent App. 17/646,330
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://patentimages.storage.googleapis.com/72/2e/0b/c80096c96acdc1/US20220368882A1.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This disclosure relates generally to method and system for draping a 3D garment on a 3D human body. Dressing digital humans in 3D have gained much attention due to its use in online shopping and draping 3D garments over the 3D human body has immense applications in virtual try-on, animations, and accurate fitment of the 3D garment is the utmost importance. The proposed disclosure is a single unified garment deformation model that learns the shared space of variations for a body shape, a body pose, and a styling garment. The method receives a plurality of human body inputs to construct a 3D skinned garments for the subject. The deep draper network trained using a plurality of losses provides efficient deep neural network based method that predicts fast and accurate 3D garment images. The method couples the geometric and multi-view perceptual constraints that efficiently learn the garment deformation’s high-frequency geometry.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">tiwari2022method</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Method and System for Draping a 3D Garment on a 3D Human Body}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tiwari, Lokender and Bhowmick, Brojeshwar}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google Patents}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{US Patent App. 17/646,330}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/prgbd_patent.gif"></div>

        <!-- Entry bib key -->
        <div id="tran2022pseudo" class="col-sm-8">
        <!-- Title -->
        <div class="title">Pseudo RGB-D for Self-Improving Monocular Slam and Depth Prediction</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://lokender.github.io" rel="external nofollow noopener" target="_blank">Lokender Tiwari</a>, Quoc-Huy Tran, Pan JI, and Manmohan Chandraker</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          Oct 2022
        </div>
        <div class="periodical">
          US Patent 11,468,585
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://patentimages.storage.googleapis.com/be/1c/b8/4bebb62c312edb/US11468585.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>A method for improving geometry-based monocular structure from motion (SfM) by exploiting depth maps predicted by convolutional neural networks (CNNs) is presented. The method includes capturing a sequence of RGB images from an unlabeled monocular video stream obtained by a monocular camera, feeding the RGB images into a depth estimation/refinement module, outputting depth maps, feeding the depth maps and the RGB images to a pose estimation/refinement module, the depths maps and the RGB images collectively defining pseudo RGB-D images, outputting camera poses and point clouds, and constructing a 3D map of a surrounding environment displayed on a visualization device.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">tran2022pseudo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Pseudo RGB-D for Self-Improving Monocular Slam and Depth Prediction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tiwari, Lokender and Tran, Quoc-Huy and JI, Pan and Chandraker, Manmohan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google Patents}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{US Patent 11,468,585}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
</div>

<hr>

<h2 id="mentoring">Mentoring  </h2>
<ul> <li><b>Thesis Mentoring</b></li> 
<ul> 
<li> <a href="https://www.linkedin.com/in/suraj-patni/" rel="external nofollow noopener" target="_blank">Suraj Patni</a> (MS Research AI) at <a href="https://home.iitd.ac.in/" rel="external nofollow noopener" target="_blank">IIT-Delhi</a>  with <a href="https://www.cse.iitd.ac.in/~chetan/" rel="external nofollow noopener" target="_blank">Prof. Chetan Arora</a>
</li>

<li>
<a href="https://www.linkedin.com/in/karantanwar/" rel="external nofollow noopener" target="_blank">Karan Tanwar</a> (B.Tech and M.Tech CSE Dual Degree) at <a href="https://home.iitd.ac.in/" rel="external nofollow noopener" target="_blank">IIT-Delhi</a> with <a href="https://www.cse.iitd.ac.in/~chetan/" rel="external nofollow noopener" target="_blank">Prof. Chetan Arora</a> </li>
<ul><li> now SDE at Amazon</li></ul>

<li>
<a href="https://anishmadan23.github.io/" rel="external nofollow noopener" target="_blank">Anish Madan</a> (B.Tech CSAM) at <a href="https://www.iiitd.edu.in/" rel="external nofollow noopener" target="_blank">IIIT-Delhi</a> with  <a href="https://faculty.iiitd.ac.in/~anands/" rel="external nofollow noopener" target="_blank"> Dr. Saket Anand</a> </li>
<ul> <li> now Research Fellow at <a href="https://www.wadhwaniai.org/" rel="external nofollow noopener" target="_blank"> Wadhwani AI</a> </li> </ul>
</ul>
</ul>

<hr>

<h2 id="talks">Professional Services (Programm Commitee Member and Reviewers)</h2>
<ul>
    <li>CVPR, ICCV, ICML, ICLR, WACV, ICME, AAAI-2023, COMSNETS-2023, CVIU, TCSVT, Elsevier Pattern Recognition, IEEE Access, IEEE Sensors</li>
</ul>

<!---  <p align="center"> <a href="mailto:tiwarilokender@gmail.com" ><img src="assets/img/gmail.png" width="45px"></a><a href="mailto:lokender.tiwari@tcs.com" ><img src="assets/img/email.png" width="45px"></a>  <a href="https://scholar.google.com/citations?user=uHfnun4AAAAJ&hl=en" ><img src="assets/img/scholar2.png" width="70px"></a> <a href="http://www.linkedin.com/in/lokender/" ><img src="assets/img/linkedin.png" width="45px"></a> <a href="https://github.com/lokender" ><img src="assets/img/github.png" width="70px"></a> <a href="https://twitter.com/LokenderTiwari" ><img src="assets/img/twitter.png" width="50px"></a> </p> --->

          </div>

          <!-- News -->
          

          <!-- Selected papers -->
          

          <!-- Social -->
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Dr. Lokender  Tiwari, Ph.D.. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7728JLJD0V"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7728JLJD0V');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
