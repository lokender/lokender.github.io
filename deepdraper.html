<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-132756458-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-132756458-1');
</script>

<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="files/jemdoc.css" type="text/css" />

<title>Lokender Tiwari</title> 

</head>
<body>


 
<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<h1 style="text-align:center">DeepDraper: Fast and Accurate 3D Garment Draping Over a 3D Human Body </h1> <br>
<h4 style="text-align:center"><a href="https://lokender.github.io/">Lokender Tiwari</a>&nbsp&nbsp&nbsp&nbsp<a href="https://sites.google.com/view/brojeshwar/home">Brojeshwar Bhowmick</a>  </h4> 
<h4 style="text-align:center"><a href="https://www.tcs.com/tcs-research">TCS Research</a> </h4>
<h4 style="text-align:center"><div>Differentiable 3D Vision and Graphics Workshop</div> </h4>
<h4 style="text-align:center"><div>In IEEE/CVF International Conference on Computer Vision (ICCV), 2021</div> </h4>



<!-- <h2 style="text-align:center">DEMOS</h2> -->
<!--   Icon Section   -->
<div class="row center">
<div class="col l8 offset-l2 m3 offset-m1 s12">
	
<h4 style="text-align:center">DeepDraper: Supplementary Video
	<p align="center">
<iframe display="block" width="640" height="320" src="https://www.youtube.com/embed/nkABDI1R320?autoplay=1"> </iframe> 
		</p>
	
<!-- <h4 style="text-align:center">KITTI Odometry Sequence 19.</h4>
	<p align="center">
<iframe display="block" width="640" height="320" src="https://www.youtube.com/embed/OOPJpHexrdE?autoplay=1"> </iframe> 
		</p>
<h4 style="text-align:center">KITTI Odometry Sequence 11.</h4>

	<p align="center">
		<iframe width="640" height="320" src="https://www.youtube.com/embed/PMYI9j5vHOw?autoplay=1"> </iframe> 
	</p>
<h4 style="text-align:center">TUM RGB-D Sequence freiburg3 Large Cabinet Validation.</h4>
<p align="center">
<iframe display="block" width="480" height="320" src="https://www.youtube.com/embed/MffXsKjy9W0?autoplay=1"></iframe>
</p> -->
</div>

</div>



<h2 style="text-align:center">ABSTRACT</h2>
<p style="text-align:justify">
Draping a 3D human mesh has garnered broad interest due to its wide applicability in virtual try-on, animations, etc. The 3D garment deformations produced by the existing methods are often inconsistent with the body shape, pose, and measurements. This paper proposes a single unified learning-based framework (DeepDraper) to predict garment deformation as a function of body shape, pose, measurements, and garment styles. We train the DeepDraper with coupled geometric and multi-view perceptual losses. Unlike existing methods, we additionally model garment deformations as a function of standard body measurements, which generally a buyer or a designer uses to buy or design perfect fit clothes. As a result,  DeepDraper significantly outperforms the state-of-the-art deep network-based approaches in terms of fitness and realism and generalizes well to the unseen style of the garments. In addition to that, DeepDraper is ~10 times smaller in size and ~23 times faster than the closest state-of-the-art method (TailorNet), which favors its use in real-time applications with less computational power. Despite being trained on the static poses of the TailorNet dataset, DeepDraper generalizes well to unseen body shapes, poses, and garment styles and produces temporally coherent garment deformations on the pose sequences even from the unseen AMASS dataset. </p>

<h2 style="text-align:center">PAPER (click on the fig below)</h2>
<div style="text-align:center"> 
<a href="https://openaccess.thecvf.com/content/ICCV2021W/Diff3D/papers/Tiwari_DeepDraper_Fast_and_Accurate_3D_Garment_Draping_Over_a_3D_ICCVW_2021_paper.pdf" target="_blank">
<img class="responsive-img" src="./papers_imgs/deepdraper.gif" width="200" >
</a>
<h4>ICCV, Workshops 2021</h4>
</div>
<div style="text-align:justify"> 
<h4>Citation</h4>
<p>Lokender Tiwari, and Brojeshwar Bhowmick, "DeepDraper: Fast and Accurate 3D Garment Draping Over a 3D Human Body", In Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV) Workshops,2021.</p>
<h4>Bibtex</h4>
<pre>@InProceedings{Tiwari_2021_ICCV,
    author    = {Tiwari, Lokender and Bhowmick, Brojeshwar},
    title     = {DeepDraper: Fast and Accurate 3D Garment Draping Over a 3D Human Body},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {October},
    year      = {2021},
    pages     = {1416-1426}
}
</pre>
</div>
<h2 style="text-align:center">DOWNLOADS</h2>
<div style="text-align:center"> 
<a href="" target="_blank">
<img class="responsive-img" src="./papers_imgs/github.png" width="100" >
</a>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp

<a href="" target="_blank">
<img class="responsive-img" src="./papers_imgs/icon_zip.png" width="100" >
</a>
<h4>Code (coming soon)  &nbsp&nbsp&nbsp&nbsp&nbsp  &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Trained Models/Results </h4>
</div>

<!-- <h2 style="text-align:center">A SELF-SUPERVISED, SELF-IMPROVING FRAMEWORK</h2>
<div style="text-align:center"> 
<img class="responsive-img" src="./papers_imgs/self_improving_framework.png" width="600" >
<p>It alternates between pose refinement (blue arrows) and depth refinement (red arrows).</p>
</div>

<h2 style="text-align:center">QUALITATIVE DEPTH RESULTS</h2>
<div style="text-align:center"> 
<img class="responsive-img" src="./papers_imgs/qual_depth_far.png" width="800" >
<p>Qualitative depth evaluation results on KITTI Odometry test set. Improvement
in depth prediction of farther scene points.</p>
<img class="responsive-img" src="./papers_imgs/qual_depth.png" width="800">
<p>Qualitative depth evaluation results on KITTI Raw Eigen split test set.
MonoDepth2-M: MonoDepth2 trained using monocular images.</p>
</div>

<h2 style="text-align:center">QUALITATIVE POSE RESULTS</h2>
<div style="text-align:center"> 
<img class="responsive-img" src="./papers_imgs/qual_pose_kitti.png" width="600" >
<p>Qualitative pose evaluation results on KITTI Odometry sequences.</p>
</div> -->

 <div id="footer">
<div id="footer-text">
<p>Last Updated July 2020.</p>
</div>
</div> 
</body>
</html>

