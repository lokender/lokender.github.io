<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-132756458-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-132756458-1');
</script>

<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="files/jemdoc.css" type="text/css" />

<title>Lokender Tiwari</title> 

</head>
<body>


 
<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<h1 style="text-align:center">Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction</h1> <br>
<h4 style="text-align:center"><a href="https://lokender.github.io/">Lokender Tiwari<sup>1</sup></a>&nbsp&nbsp&nbsp&nbsp<a href="https://sites.google.com/site/peterji1990">Pan Ji<sup>2</sup></a>&nbsp&nbsp&nbsp&nbsp<a href="https://cs.adelaide.edu.au/~huy/home.php">Quoc-Huy Tran<sup>2</sup></a>&nbsp&nbsp&nbsp&nbsp<a href="https://bbzh.github.io/">Bingbing Zhuang<sup>2</sup></a>&nbsp&nbsp&nbsp&nbsp<a href="https://www.iiitd.edu.in/~anands/index.html">Saket Anand<sup>1</sup></a>&nbsp&nbsp&nbsp&nbsp<a href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker<sup>2,3</sup></a>  </h4> 
<h4 style="text-align:center"><a href="https://www.iiitd.ac.in/"><sup>1</sup>IIIT-Delhi</a>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="http://www.nec-labs.com/research-departments/media-analytics/media-analytics-home"><sup>2</sup>NEC Labs America </a>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://ucsd.edu/"><sup>3</sup>UCSD</a>  </h4>

<h4 style="text-align:center"><div>In European Conference on Computer Vision (ECCV), 2020</div> </h4>



<h2 style="text-align:center">DEMOS</h2>
<!--   Icon Section   -->
<div class="row center">
<div class="col l8 offset-l2 m3 offset-m1 s12">
<h4 style="text-align:center">KITTI Odometry Sequence 19.</h4>
	<p align="center">
<iframe display="block" width="640" height="320" src="https://www.youtube.com/embed/OOPJpHexrdE?autoplay=1"> </iframe> 
		</p>
<h4 style="text-align:center">KITTI Odometry Sequence 11.</h4>

	<p align="center">
		<iframe width="640" height="320" src="https://www.youtube.com/embed/PMYI9j5vHOw?autoplay=1"> </iframe> 
	</p>
<h4 style="text-align:center">TUM RGB-D Sequence freiburg3 Large Cabinet Validation.</h4>
<p align="center">
<iframe display="block" width="480" height="320" src="https://www.youtube.com/embed/MffXsKjy9W0?autoplay=1"></iframe>
</p>
</div>

</div>



<h2 style="text-align:center">ABSTRACT</h2>
<p style="text-align:justify">
Classical monocular Simultaneous Localization And Mapping (SLAM) and the recently emerging convolutional neural networks (CNNs) for monocular depth prediction represent two largely disjoint approaches towards building a 3D map of the surrounding environment. In this paper, we demonstrate that the coupling of these two by leveraging the strengths
of each mitigates the others shortcomings. Specifically, we propose a joint narrow and wide baseline based self-improving framework, where on the one hand the CNN-predicted depth is leveraged to perform pseudo RGB-D feature-based SLAM, leading to better accuracy and robustness than the monocular RGB SLAM baseline. On the other hand, the bundleadjusted 3D scene structures and camera poses from the more principled geometric SLAM are injected back into the depth network through novel wide baseline losses proposed for improving the depth prediction network,
which then continues to contribute towards better pose and 3D structure estimation in the next iteration. We emphasize that our framework only requires unlabeled monocular videos in both training and inference stages, and yet is able to outperform state-of-the-art self-supervised monocular and stereo depth prediction networks (e.g., Monodepth2) and
feature-based monocular SLAM system (i.e., ORB-SLAM). Extensive experiments on KITTI and TUM RGB-D datasets verify the superiority of our self-improving geometry-CNN framework.
</p>

<h2 style="text-align:center">PAPER</h2>
<div style="text-align:center"> 
<a href="https://arxiv.org/abs/2004.10681" target="_blank">
<img class="responsive-img" src="./papers_imgs/prgbd_paper.png" width="100" >
</a>
<h4>ECCV 2020</h4>
</div>
<div style="text-align:justify"> 
<h4>Citation</h4>
<p>Lokender Tiwari, Pan Ji, Quoc-Huy Tran, Bingbing Zhuang, Saket Anand and Manmohan Chandraker, "Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction", In Proceedings of European Conference on Computer Vision, 2020.</p>
<h4>Bibtex</h4>
<pre>@inproceedings{zou2018dfnet,
    author={Tiwari, Lokender and Ji, Pan and Tran, Quoc-Huy and Zhuang, Bingbing and Anand, Saket 
 	and Chandraker, Manmohan},
    title     = {Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction}, 
    booktitle = {European Conference on Computer Vision},
    year      = {2020}
}
</pre>
</div>
<h2 style="text-align:center">DOWNLOADS</h2>
<div style="text-align:center"> 
<a href="" target="_blank">
<img class="responsive-img" src="./papers_imgs/github.png" width="100" >
</a>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp

<a href="" target="_blank">
<img class="responsive-img" src="./papers_imgs/icon_zip.png" width="100" >
</a>
<h4>Code (coming soon)  &nbsp&nbsp&nbsp&nbsp&nbsp  &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Trained Models/Results </h4>
</div>

<h2 style="text-align:center">A SELF-SUPERVISED, SELF-IMPROVING FRAMEWORK</h2>
<div style="text-align:center"> 
<img class="responsive-img" src="./papers_imgs/self_improving_framework.png" width="600" >
<p>It alternates between pose refinement (blue arrows) and depth refinement (red arrows).</p>
</div>

<h2 style="text-align:center">QUALITATIVE DEPTH RESULTS</h2>
<div style="text-align:center"> 
<img class="responsive-img" src="./papers_imgs/qual_depth_far.png" width="800" >
<p>Qualitative depth evaluation results on KITTI Odometry test set. Improvement
in depth prediction of farther scene points.</p>
<img class="responsive-img" src="./papers_imgs/qual_depth.png" width="800">
<p>Qualitative depth evaluation results on KITTI Raw Eigen split test set.
MonoDepth2-M: MonoDepth2 trained using monocular images.</p>
</div>

<h2 style="text-align:center">QUALITATIVE POSE RESULTS</h2>
<div style="text-align:center"> 
<img class="responsive-img" src="./papers_imgs/qual_pose_kitti.png" width="600" >
<p>Qualitative pose evaluation results on KITTI Odometry sequences.</p>
</div>

 <div id="footer">
<div id="footer-text">
<p>Last Updated July 2020.</p>
</div>
</div> 
</body>
</html>
